---
title: "Metamodel performance metrics"
output: html_notebook
---

\newcommand{\Gamm}{\Gamma}
\newcommand{\biais}{\nu}
\newcommand{\Ytrue}{{Y}}
\newcommand{\Ypred}{\hat{Y}}
\newcommand{\erreurprobarelative}{\epsilon_P^{MM}}


This notebook evaluates the precision of the metamodel on the evaluation of the probabilities, by computing the relative probability error.

## Introduction 

The method is to : 
 
  - Sample $(\tilde{x}^k)_{k \in \{1,\dots,10^5\}}$ i.i.d of density function $\biais$
  - Compute $Y(\tilde{x}^k)_{k \in \{1,\dots,10^5\}}$ the sample of real maps
  - Compute $\hat{Y}(\tilde{x}^k)_{k \in \{1,\dots,10^5\}}$ the sample of predicted maps from 1300 training maps
  - Compute $(\Gamm^r)_{r \in \{1,\dots,100\}}$ from the $\Gamm^{\star}$ obtained by applying the Prototype Maps Algorithm with 10^6 maps (notebook lloyd_true.Rmd). A random change between 0 and $20\%$ is apply to $\Gamm^{\star}$
  - Compute $\forall r \in \{1,\dots,100\}, \erreurprobarelative(n,\Gamm^{r},j) = \frac{\mid  \hat{P}_{n}(\Gamm^{r},j,\Ytrue) - \hat{P}_{n}(\Gamm^{r},j,\Ypred) \mid}{\hat{P}_{n}(\Gamm^{r},j,\Ytrue)}$ with $n= 10^5$
  
 
  

```{r}
source.all("GpOutput2D-main")
source.all("FunQuant-0.1.3/R")
source("Campbell2D.R")
source("Campbell_utils.R")
```



```{r}
dim_map = c(64,64)

z1 = seq(-90,90,l=dim_map[1])
z2 = seq(-90,90,l=dim_map[2])

```

## Get biased sample 

We sample $(\tilde{x}^k)_{k \in \{1,\dots,10^5\}}$ i.i.d of density function $\biais$

```{r}
set.seed(1234)
n=10^5
Xtilde = matrix(runif(n =5*n), ncol = 5)
Xtilde = cbind(Xtilde, sample(1:10,size = n, replace = TRUE) / 10) 

unif = runif(n) 
Xtilde = cbind(Xtilde, (unif <= 8/13)*runif(n))

Xtilde = cbind(Xtilde, rep(-1,n))
Xtilde = transform_X(Xtilde)
X_training = Xtilde[1:1300,]
Y_training = Campbell2D(X_training,z1,z2)
```

## Compute the weights 

We compute $\left(\frac{f_{X}(\tilde{x}^k)}{\biais(\tilde{x}^k)}\right){_{k \in \{1,\dots,10^5\}}}$

```{r}
Xtilde_phy = ((Xtilde + 1)/6)[,1:7]
for(i in 1:5){
  Xtilde_phy[,i] = Xtilde_phy[,i] * (df_bornes[i,2]-df_bornes[i,1]) + df_bornes[i,1]
}
Xtilde_phy[,6] = Xtilde_phy[,6]*10

registerDoParallel(cores = 24)

density_ratio = compute_density_ratio(fX, g, Xtilde_phy)

```

## Get $\Gamm^{\star}$ from the notebook PMalgo_true.Rmd

```{r}
load("lloyd_res_true.RData")
gamma_star = lloyd_res_true$gamma #the prototype maps obtained with the true maps



```


## Tuning of the hyperparameters

We will perform a cross validation step to tune the hyperparameters $\tilde{K}$ and $n_{pc}$ of the FPCA. 

For all $\tilde{K}$ and $n_{pc}$, we compute : $\forall j \in {1,\dots, 5},\erreurprobarelative(10^4,\Gamm^{\star},j)$. 

And then we select the pair $\tilde{K}, n_{pc}$ that minimizes the mean of these 5 relative error.

```{r}
model_cv = create_models_tuning(outputs = Y_training, ncoeff_vec = seq(1000,4000, 500), npc = 6, formula = ~1, design = data.frame(x = X_training)[,1:7],control = list(trace = 0), nugget.estim = TRUE)
```


```{r}
npc_vec = 2:6 #These are the testes values for npc
ncoeff_vec = seq(1000,4000, 500)
n_cv = 10^4 
df_cv_pred = data.frame()
Y_test = Campbell2D(Xtilde[1301:(n_cv+1300),],z1,z2) #we compute 10^4 maps

perf_probas = probas_training_test(outputs_train = Y_training, outputs_test = Y_test, density_ratio = density_ratio[1301:(1300+n_cv)], gamma = gamma_star, model_tuning = model_cv, ncoeff_vec = ncoeff_vec, npc_vec = npc_vec, design_train = data.frame(x = X_training)[,1:7], design_test =  data.frame(Xtilde[1301:(n_cv+1300),1:7]))

errors = perf_probas$error

best_params = errors[,1:2][which.min(apply(errors[,3:7], 1, mean)),]
best_params
```

## Creation of the family $(\Gamma_{r})_{r \in \{1,\dots,100\}}$

```{r}
set.seed(2)
gamma_it_list = list()
for(it in 1:100){
  gamma_it_list[[it]] = lapply(1:5, function(i){gamma_star[[i]]*matrix(runif(64^2,0.8,1.2), ncol = 64, nrow = 64)})
}
```

## True maps

Compute $Y(\tilde{x}^k)_{k \in \{1,\dots,10^5\}}$ the sample of real maps

```{r}
maps_true = Campbell2D(Xtilde,z1,z2)

```


Compute the probabilities for each $\Gamm^{r}$

```{r}
proba_true = lapply(1:length(gamma_it_list), function(it){get_probas(density_ratio = density_ratio[1:10^5], outputs = maps_true[,,1:10^5],gamma = gamma_it_list[[it]])})

rm(maps_true)
```

## Predicted maps

Predict the $10^5$ maps with the tuned hyperparameters 

```{r}
registerDoParallel(cores = 24)

metamodel = fit_metamodel(design_train = data.frame(x = X_training)[,1:7], outputs_train = Y_training, seed = 1, ncoeff = 2000, npc = 6,control = list(trace = 0))


```

```{r}
set.seed(1234)

maps_metamodel = predict_outputs(metamodel_fitted = metamodel, design_test = data.frame(Xtilde[,1:7])) #predict the maps

maps_metamodel[,,1:1300] = Y_training

```

Compute the predicted probabilities

```{r}
proba_predict = lapply(1:length(gamma_it_list), function(it){get_probas(density_ratio = density_ratio[1:10^5], outputs = maps_metamodel[,,1:10^5],gamma = gamma_it_list[[it]])})

rm(maps_metamodel)

```

## Distribution of the errors 

Compute the relative probability error and display the distribution 


```{r, fig.height = 5, fig.width = 5}
relative_error = data.frame(relative_error = abs(unlist(proba_predict) - unlist(proba_true))/unlist(proba_true))

boxplot(relative_error, col = "darkolivegreen2", ylab = "Proba. relative metamodel error", log = "y",  cex.lab=1.3, cex.axis = 1.3)

```





